#!/usr/bin/Rscript

packages <- c("caret", "lubridate", "xlsx", "rJava","doSNOW", "reshape", "randomForest",
              "foreach","doParallel","gbm","openxlsx","teradataR","RODBC","digest","sendmailR")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}
#### IMPORTANT NOTE
#### Filepaths will likely have to be specified depending on
#### where this program is run. The filepaths below may need
#### to be checked
library(caret)
library(doParallel)
library(xlsx)
library(sendmailR)
from <- "<samuel.tisi@walmart.com>"
to <- "<Zhe.Li@walmart.ca>"
subject <- "Walmart Preditive Modelling has begun"
body<-("Model has began to run ")
mailControl=list(smtpServer="smtp-gw1.wal-mart.com")
#sendmail(from=from,to=to,subject=subject,msg=body,control=mailControl)
#source('//cants802/share2/SHARE/Paul Rizk/Useful Files/Functions List.R')
setwd("//cants802/share2/SHARE/Logistics Analytics/Forecast")
getSystem = function(){return(Sys.info()['sysname'])}

print("Beginning program")

####################################
### Function Definitions Section ###
####################################

# These functions compartmentalize additional read in and formats from
# other sources that were tried. Some are used within the readInFormat1 function. 
# Only readInFormat7088 and readInFormat6064_6081 are used by default in
# the final program.

# Note: These are actually called in the readInFormat() function, so look there to understand context

readInFormat7088 = function(file = "//cants802/share2/SHARE/Logistics Analytics/Forecast/Data/PDC7087and6080/Forecast (2).xls"){
  data = xlsx::read.xlsx2(file = file, sheetIndex = 3, colIndex = 1:3, colClasses = c('Date','numeric','numeric'))
  names(data) = c('date','SSTK','PBYL')
  data = reshape2::melt(data, id.vars = 'date')
  data$dc = 7088
  names(data)[names(data) %in% c('variable','value')] = c('channel','volume')
  data = data[,c('date','dc','channel','volume')]
  data = data[complete.cases(data),]
  return(data)
}

readInFormat6064_6081 = function(file = "//cants802/share2/SHARE/Logistics Analytics/Forecast/Data/PDC7087and6080/6081 Forecast.xls"){
  data = xlsx::read.xlsx2(file = file, sheetIndex = 3, colIndex = 1:2, colClasses = c('Date','numeric','numeric'))
  names(data) = c('date','SSTK')
  data = reshape2::melt(data, id.vars = 'date')
  data$dc = 6081
  names(data)[names(data) %in% c('variable','value')] = c('channel','volume')
  data = data[,c('date','dc','channel','volume')]
  data = data[complete.cases(data),]
  return(data)
}

# This function is used to read in the main data source from an xlsx file
# located in \\cadfs\\logistics. The file contains dates, channels, dcs and case volumes
# as reported to CA Logistics every morning through email. The file is updated manually
# by Paula or Val. Most of this function involves trimming this file down to
# the bare necessities as well as some formatting

readInFormat1 = function(filename = "//cadfs/logistics/Logistics/PDC Forecast/Forecasting/Forecast VS. Actual 2014 - New 2014 val.xlsm", 
                         clip6601 = T, 
                         remove6064 = T, 
                         includePredictions = F, 
                         replace7088 = T, 
                         replace6081 = T)
{
  if(includePredictions){
    data1 = xlsx::read.xlsx2(filename, sheetIndex = 2, colIndex = c(1:5), colClasses = c('Date','character','character','numeric','numeric'))
  }
  else{
    data1 = xlsx::read.xlsx2(filename, sheetIndex = 2, colIndex = c(1:3,5), colClasses = c('Date','character','character','numeric'))    
  }
  names(data1) = tolower(names(data1))
  names(data1)[names(data1)=="actual...appt.log"] = "volume"
  data1 = data1[data1$channel != 'PBYL' | data1$dc != 6081,]
  data1 = subset(data1, subset = lubridate::year(data1$date) != 2005)
  data1$date[data1$date == "2105-04-12"] = as.Date('2015/4/12')
  data1$dc = as.character(data1$dc)
  data1 = data1[!duplicated(data1),]
  if(clip6601){
    data1 = data1[data1$dc != 6601 | data1$channel != 'PBYL' | as.Date(data1$date, format = '%Y-%m-%d') > as.Date("2013-11-13", format = '%Y-%m-%d'),]
  }
  if(remove6064){
    data1 = subset(data1, dc != 6064)
  }
  if(replace7088 & !includePredictions){
    add = readInFormat7088()
    data1 = merge(data1, add, c('date','dc','channel'), all = T)
    data1$volume.x[data1$dc == 7088 & is.na(data1$volume.x)] = data1$volume.y[data1$dc == 7088 & is.na(data1$volume.x)] 
    data1$volume.y = NULL
    names(data1)[names(data1) == 'volume.x'] = 'volume'
  }
  if(replace7088 & includePredictions){
    data1 = merge(data1, readInFormat7088(), by = c('date','dc','channel'), all = T)
    data1$volume.x[data1$dc == 7088] = data1$volume.y[data1$dc == 7088]
    data1$volume.y = NULL
    names(data1)[names(data1) == 'volume.x'] = 'volume'
    data1$forecast = round(data1$forecast)
    data1$volume = round(data1$volume)
  }
  if(replace6081 & !includePredictions){
    add = readInFormat6064_6081()
    data1 = merge(data1, add, c('date','dc','channel'), all = T)
    data1$volume.x[data1$dc == 6081 & is.na(data1$volume.x)] = data1$volume.y[data1$dc == 6081 & is.na(data1$volume.x)] 
    data1$volume.y = NULL
    names(data1)[names(data1) == 'volume.x'] = 'volume'
  }
  if(replace6081 & includePredictions){
    data1 = merge(data1, readInFormat6081(), by = c('date','dc','channel'), all = T)
    data1$volume.x[data1$dc == 6081] = data1$volume.y[data1$dc == 6081]
    data1$volume.y = NULL
    names(data1)[names(data1) == 'volume.x'] = 'volume'
    data1$forecast = round(data1$forecast)
    data1$volume = round(data1$volume)
  }
  data1 = data1[order(data1$dc, data1$channel, data1$date),]
  return(data1)
}



## These functions are just for feature generation

## This merges the DC-store alignments data set "alignments" with the main data set
addVariables_alignments = function(data){
  data = merge(data, alignments, by = c('date','dc'), all.x = T)
  return(data)
}  

## This just adds a bunch of date-based features (i.e. weekday, day of month etc)
addVariables_dateBased = function(data)
{ 
  data$yearDay = lubridate::yday(data$date)
  data$monthDay = lubridate::mday(data$date)
  data$weekDay = lubridate::wday(data$date)
  data$weekDaySin1 = sin(data$weekDay * 2 * pi / 7)
  data$weekDayCos1 = cos(data$weekDay * 2 * pi / 7)
  data$yearWeek = lubridate::week(data$date)
  return(data)
}

## This adds a feature that is the number of days to the nearest holiday.
## Note this could be days past or until the nearest holiday
## Needs to be updated with 2016 values *****
addVariables_proximity = function(data){
  
  # List of Ontario statuatory holidays from 2012 to 2015
  statHolidays = c('january 2 2012','february 20 2012','april 6 2012','may 21 2012','july 2 2012','september 3 2012',
                   'october 8 2012','december 25 2012','december 26 2012','august 6 2012','january 1 2013','february 16 2013',
                   'march 29 2013','may 20 2013','july 1 2013','september 2 2013','october 14 2013', 'december 25 2013',
                   'december 26 2013','august 5 2013','january 1 2014','february 17 2014','april 18 2014','may 19 2014',
                   'july 1 2014','august 4 2014','september 1 2014','october 13 2014','december 25 2014','december 26 2014',
                   'january 1 2015','february 16 2015','april 3 2015','may 18 2015','july 1 2015','august 3 2015',
                   'september 7 2015','october 12 2015','december 25 2015','december 26 2015','january 1 2016','february 15 2016',
                   'march 25 2016','may 23 2016','july 1 2016','august 1 2016','september 5 2016',
                   'october 10 2016','december 25 2016','december 26 2016'
  )
  statHolidays = strptime(statHolidays, format = '%b %d %Y')
  
  data$proximity = NA
  
  for(i in 1:nrow(data)){
    proximities = difftime(data$date[i], statHolidays,units = "days")
    data$proximity[i] = as.numeric(proximities[which(abs(proximities) == min(abs(proximities)))][1])
  }
  return(data)
}

# Simple missing value imputation by local polynomial smoother
imputeMissing = function(object){
  loess = loess(object$volume ~ as.numeric(object$date))
  missing = which(is.na(object$volume))
  if(length(missing) != 0){
    object$volume[missing] = round(predict(loess, newdata = object)[missing])
  }
  return(object)
}

# This function gets predictions from two models and averages them
getPreds = function(data, model1, model2){
  pred1 = predict(model1, newdata = data)
  pred2 = predict(model2, newdata = data)
  pred = rowMeans(cbind(pred1, pred2), na.rm = T)
  return(pred)
}


# Function for easy recoding of multiple values
recode1 = function(x, start, end, else.case = c('missing','original')) 
{
  # Set code for intentional conversion to NA
  end[is.na(end)] = 'MISSING'
  out = as.vector(end[match(x,start)])
  if(else.case[1]=='missing'){out = out; out[out=='MISSING'] = NA}
  else if(else.case[1]=='original'){out[is.na(out)]=x[is.na(out)]; out[out=='MISSING'] = NA}
  else {out[is.na(out)]=else.case[1]; out[out=='MISSING'] = NA}
  return(out)
}

# Function to extract Walmart, fiscal-calendar elements from a date
# Depends on the modified calendar file loaded below located in the project
# folder "Data" directory

load("./Data/wmCalendar")

toWalmart = function(date, to = 'day'){
  if(to %in% c('week', 'dayWeek', 'quarter','dayQuarter','year','dayYear','wm_yr_wk') == 0){
    stop("'to' arguement must be one of 'dayWeek','week','quarter','dayQuarter','year', 'dayYear' or 'wm_yr_wk'")
  }
  if(to == 'dayWeek'){
    return(as.numeric(recode1(date, wmCalendar$date, wmCalendar$day_of_wk)))
  }
  else if(to == 'week'){
    return(as.numeric(recode1(date, wmCalendar$date, wmCalendar$wm_week)))
  }
  else if(to == 'quarter'){
    return(as.numeric(recode1(date, wmCalendar$date, wmCalendar$wm_qtr)))
  }
  else if(to == 'dayQuarter'){
    return(as.numeric(recode1(date, wmCalendar$date, wmCalendar$fiscal_dayQuarter)))
  }
  else if(to == 'year'){
    return(as.numeric(recode1(date, wmCalendar$date, wmCalendar$fiscal_year)))
  }
  else if(to == 'dayYear'){
    return(as.numeric(recode1(date, wmCalendar$date, wmCalendar$fiscal_day)))
  }
  else if(to == 'wm_yr_wk'){
    return(as.numeric(recode1(date, wmCalendar$date, gsub(',','',wmCalendar$wm_yr_wk))))
  }
}



#############################################
### Preamble: Getting DC-Store Alignments ###
#############################################

# This section will pull the WHSE_ALIGN table from Teradata.
# The table gives information on current and historic DC-store alignments 
# for different item groups (alignment types). This is converted to counts
# of the number of stores serviced by each DC for each of the item groups
# (alignment types). These counts are used as features to fit the models.

# Connect to and query teraalignments WHSE_ALIGN table to get DC-store alignments
# Note: Will need to change Teradata login info on the next line
library(RODBC)
library("digest")
source("./Data/crypt.R")
load("//cants5010/Logistics/Logistics Analytics/key.RData")
credentials <- read.aes(filename = "./Data/credentials.txt",key = key)
ocn <- odbcConnect("alignment", uid = credentials[1,1], pwd = credentials[1,2])
alignments <- sqlFetch(ocn, "WHSE_ALIGN")
write.csv(alignments, file = paste('./Output/alignment', as.Date(base::date(),''),'.csv',sep=''), row.names = F)


# Format whse_align 
names(alignments) = tolower(names(alignments))

alignments$align_date = as.Date(alignments$align_date, format = '%Y-%m-%d')
alignments$expire_date = as.Date(alignments$expire_date, format = '%Y-%m-%d')
alignments$division_nbr = NULL
alignments$store_nbr = NULL

alignments = alignments[order(alignments$whse_nbr, alignments$align_date),]

# Setting up a range of dates as a template and replacing any alignment dates prior
# to this with the minimum date
dates = seq.Date(from = as.Date('2001/1/30'), to = max(alignments$expire_date[alignments$expire_date!='2500-01-01']), by = 1)
alignments$align_date[alignments$align_date < as.Date('2001/1/30')] = as.Date('2001/1/30')

names(alignments) = c('dc','align','type','expire')

# This will go through each DC-alignment type combination and count the number 
# of stores that have that DC-alignment type combination for each day 

alignments = split(alignments, interaction(alignments$dc, alignments$type, drop = T))
alignments = lapply(alignments, function(i) {
  i = reshape2::melt(i, measure.vars = c('align','expire'))
  names(i)[names(i)=='value'] = 'date'
  i$number = as.numeric(ifelse(i$variable == 'align', 1, -1))
  i = reshape2::dcast(i, dc + type + date ~ ., fun.aggregate = sum, value.var = 'number')
  names(i) = c('dc','type','date','count')
  i$date = as.Date(i$date, '%Y-%m-%d')
  return(i)
})

alignments = lapply(seq_along(alignments), function(i) {
  splits = strsplit(names(alignments)[i], '\\.')
  i = merge(data.frame(date = dates), alignments[[i]], by = 'date', all.x = T)
  i$dc = splits[[1]][1]
  i$type = splits[[1]][2]
  i$count[is.na(i$count)] = 0
  i$alignments = cumsum(i$count)
  i$count = NULL
  i = subset(i, date >= as.Date('2010/2/1') & date <= as.Date('2018/2/1'))
  return(i)
})

alignments = do.call(rbind, alignments)
alignments = reshape2::dcast(alignments, date + dc ~ type, fun.aggregate = sum, value.var = 'alignments')
alignments = alignments[order(alignments$dc, alignments$date),]

# Also calculate a total column (i.e. number of stores a DC is aligned to for any alignment type
# on a given day)
alignments$total = rowSums(subset(alignments, select = -c(date,dc)))

# This bit makes some rough historic adjustments for DCs 6064P and 6081
# 6081 alignments are 6064 (not 6064P) alignments for SSTK prior to May 2015
# 6064P alignments can't be separated out from 6064 alignments because whoever 
# created the Teraalignments table obviously didn't think that was important
alignments6064 = alignments[alignments$dc == 6064,]
alignments6081 = alignments[alignments$dc == 6081,]
alignments6064$total = NULL
alignments6081$total = NULL
alignments6081[1:1939,-c(1:2)] = alignments6081[1:1939,-c(1:2)] + alignments6064[1:1939,-c(1:2)] 
alignments6064$total = rowSums(alignments6064[,-c(1:2)])
alignments6081$total = rowSums(alignments6081[,-c(1:2)])
alignments6081$dc = 6081

alignments[alignments$dc==6081,] = alignments6081
alignments$dc[alignments$dc == 6064] = '6064P'

# This will extend the final alignments alignmentsset into the future 500 days.
# It will simply assume that there are no changes to any alignments until
# some actually get recorded.
alignments = split(alignments, alignments$dc)
alignments = lapply(alignments, function(i) {
  n = 500
  add = do.call(rbind, replicate(n, i[nrow(i),], simplify = F))
  add$date = seq.Date(from = max(i$date)+1, length.out = n, by = 1)
  i = rbind(i,add)
  return(i)
})

alignments = do.call(rbind, alignments)


#########################
### Main Program Body ###
#########################
print("Preamble complete, beginning main program: data reading")
### Data Read in and Munge Section ###

data = readInFormat1()
data=data[data$dc==6064|data$dc==6080|data$dc==7072|data$dc==7087|data$dc==6601
          |data$dc==7088|data$dc==6081|data$dc==6094|data$dc=='6064P',]

# A separate model will be fit for each DC-Channel combination so
# here we are splitting up the data set by DC and channel.

data = split(data, interaction(data$dc, data$channel, drop = T))

# 6064 is no longer active so we're just removing that data here

data = data[names(data) %in% c('6064.PBYL','6064.SSTK') == 0]
names = names(data)

# This bit is just feature generation. This set of features works well but
# could certainly be improved/streamlined with some work

data = lapply(seq_along(data), function(i){
  data[[i]]= data[[i]][order(data[[i]]$date),]
  
  # Removing some outliers. Should be careful here and maybe check every 
  # once in a while to make sure new (valid) data points haven't been 
  # removed by these rules
  
  data[[i]]$volume[data[[i]]$volume>40000 & data[[i]]$dc == 7088 & data[[i]]$channel == 'PBYL'] = NA
  data[[i]]$volume[data[[i]]$volume>150000 & data[[i]]$dc == 6081 & data[[i]]$channel == 'SSTK'] = NA
  data[[i]]$volume[data[[i]]$volume<25000 & data[[i]]$dc == 7088 & data[[i]]$channel == 'SSTK'] = NA
  data[[i]]$volume[data[[i]]$volume<20000 & data[[i]]$dc == 6094 & data[[i]]$channel == 'SSTK'] = NA
  data[[i]]$volume[data[[i]]$volume<20000 & data[[i]]$dc == 7087 & data[[i]]$channel == 'SSTK'] = NA
  data[[i]]$volume[data[[i]]$volume == 0] = NA
  
  # This imputes missing values using a local polynomial smoother
  
  data[[i]] = imputeMissing(data[[i]])
  
  # These bits just add various features. Mostly based on date (this is
  # unfortunately all we really have) but also based on DC-store aliugnments.
  
  data[[i]]= addVariables_dateBased(data[[i]])
  data[[i]]= addVariables_proximity(data[[i]])
  data[[i]]= addVariables_alignments(data[[i]])
  
  # Additional date-based features using walmart calendar instead of normal calendar
  
  data[[i]]$wmday = toWalmart(data[[i]]$date, 'dayYear')
  data[[i]]$wmweek = toWalmart(data[[i]]$date, 'week')
  data[[i]]$wmquarter = toWalmart(data[[i]]$date, 'quarter')
  data[[i]]$wmdayquarter = toWalmart(data[[i]]$date, 'dayQuarter')
  data[[i]]$year = lubridate::year(data[[i]]$date)
  
  data[[i]] = data[[i]][complete.cases(data[[i]]),]
  return(data[[i]])
})
data=unique(data)
names(data) = names
data_all=rbind(data[[1]],data[[2]],data[[3]],data[[4]],data[[5]],data[[6]],
      data[[7]],data[[8]],data[[9]],data[[10]],data[[11]])

# This bit just registers a parallel backend depending on the OS.
# Uses doMC package for UNIX and doParallel package for Windows
# IMPORTANT NOTE 
# Set the number of cores based on the hardware you are using.

if(getSystem()=='Darwin'){
  # For UNIX
  doMC::registerDoMC(16)
}else{
  # For Windows
  cl = parallel::makePSOCKcluster(6)
  doParallel::registerDoParallel(cl)}



### Model Fitting Section ###
print("Data reading complete, beginning model training")
# This section is the workhorse of the tool. 
# It fits two (currently) predictive models using supervised learning algorithms
# Model 1: Gradient-Boosted Decision Tree (R-package gbm)
# Model 2: Random Forest (R-package randomForest)
# Both models are fit and tuned using the caret package. Models are refit
# and re-tuned each morning. 
# THOUGHT: I've never gotten around to it but fitting and tuning the models every morning may actually be reducing 
# accuracy so there may be some room for improvement by reducing the schedule of
# refit or re-tune or both. 

# This is just setting up some control parameters for the tuning and the model formulas.
# Tuning uses 10-fold cross-validation. Probably better to use repeated CV or bootstrap on
# a faster system (Should help with the variance problem due to frequent refitting)

control = trainControl(method = 'cv', number = 10, savePredictions = T, verboseIter = T)
formula1 = formula("volume ~ yearDay + monthDay + weekDay + weekDaySin1 + weekDayCos1 + yearWeek + proximity + AB + AC + AP + AS + CI + DI + DS + E1 + E2 + E3 + FA + FS + GB + GC + GD + GF + GM + GP + GV + GY + HA + HW + JW + N1 + N2 + N3 + N4 + N5 + N6 + N7 + N8 + N9 + OC + PH + SB + SH + SP + SS + TD + TS + WH + WP + total + wmday + wmweek + wmquarter + wmdayquarter + year")

fitsGbm = lapply(seq_along(data), function(i) {
  fit = train(formula1 , method = 'gbm', data = subset(data[[i]], select = -c(dc,channel)), trControl = control, verbose = F, bag.fraction = 1,
              tuneGrid = expand.grid(n.trees = seq(10,500,10), interaction.depth = 1:12, shrinkage = 0.1, n.minobsinnode = c(13,10,7,4)))
})

fitsRf = lapply(seq_along(data), function(i) {
  fit = train(formula1, method = 'rf', data = subset(data[[i]], select = -c(dc,channel)), trControl = control,
              ntree = 500, do.trace = 5, tuneLength = 10)
})
print("Training complete, finalizing output")
names(fitsRf) = names(data)
names(fitsGbm) = names(data)
fits = list(fitsGbm = fitsGbm, fitsRf = fitsRf)


### Forecasting/Prediction Section ###

# This section simply uses the model to produce predictions for until
# the end of the fiscal year (this can be extended if you like)

# Create a data set to be used for prediction. i.e. This will have, for all DCs and
# Channels, a range of dates from tomorrow to the end of January. 
# This data set will not have volumes (these will be predicted)
dataForecast = expand.grid(
  date = seq.Date(from = as.Date(base::date(),'')+1, to = as.Date('2017/1/31'), by = 1)
)
dataForecast = merge(dataForecast, unique(do.call(rbind,data)[,c('dc','channel')]))


# This bit will create the same features from the template dataset as are found (and needed)
# in the fit models
dataForecast= addVariables_dateBased(dataForecast)
dataForecast= addVariables_proximity(dataForecast)
dataForecast= addVariables_alignments(dataForecast)
dataForecast$wmday = toWalmart(dataForecast$date, 'dayYear')
dataForecast$wmweek = toWalmart(dataForecast$date, 'week')
dataForecast$wmquarter = toWalmart(dataForecast$date, 'quarter')
dataForecast$wmdayquarter = toWalmart(dataForecast$date, 'dayQuarter')
dataForecast$year = lubridate::year(dataForecast$date)

dataForecast = split(dataForecast, f = interaction(dataForecast$dc, dataForecast$channel, drop = T))

# This is the part that actually produces the predictions. It adds them to the existing 
# dataForecast data sets as a variable called "volume" (like in the data used to create the models)
dataForecast = lapply(seq_along(dataForecast), function(i) {
  dataForecast[[i]]$volume = getPreds(dataForecast[[i]], fits$fitsGbm[[i]], fits$fitsRf[[i]])
  return(dataForecast[[i]])})

# Write final, unmodified, predictions to csv

finalPredictions = do.call(rbind, dataForecast)
finalPredictions = reshape2::dcast(finalPredictions, date ~ dc + channel, fun.aggregate = sum , value.var = 'volume')

####################### WRITE OUT LINE #########################
write.csv(finalPredictions, file = paste('./Output/testforecastsFrom', as.Date(base::date(),''),'.csv',sep=''), row.names = F)
################################################################



### Output to Tableau Section ###

# Overall this section will read in the current xlsx file, overwrite the predictions
# for tomorrow onwards with the new predictions and then write the xlsx file again
dataOld = openxlsx::read.xlsx(xlsxFile = "//cants802/share2/SHARE/Logistics/Operations Planning Forecasts/Forecast.xlsx", sheet = 1, detectDates = T)

# The overwrite will be done by merging the old set with the new set.
# Since the new set will only have predictions from tomorrow onwards old
# values for dates older than that will be NA

# This is separating out the relevant bits of dataForecast 
# (i.e. dc, channel, date and volume) for the merge
newPredictions = do.call(rbind, dataForecast)
newPredictions = subset(newPredictions, select = c(date,dc,channel,volume))
newPredictions$volume = round(newPredictions$volume)
newPredictions$dc[newPredictions$dc == '6064P'] = '6064'

# This is the merge itself
dataNew = merge(dataOld, newPredictions,
                by.x = c('TY_date','dc','channel'),
                by.y = c('date', 'dc', 'channel'), all.x = T)

# We now replace any value of "TY_forecast" in dataNew with the value of 
# "volume" in dataNew if the record is for tomorrow or later, otherwise 
# leave it as is
dataNew$TY_forecast = ifelse(dataNew$TY_date > as.Date(base::date(),''),
                             dataNew$volume, dataNew$TY_forecast)

dataNew$volume = NULL

# This is just adding in the most recent actuals. They are updated in the original
# xlsx file (i.e. the one originally used to get the data for model fitting).
# Do a similar merge as for the predictions except just overwrite the entire
# column.
actuals = readInFormat1()
actuals$dc[actuals$dc == '6064P'] = '6064'
dataNew = merge(dataNew, actuals,
                by.x = c('TY_date','dc','channel'),
                by.y = c('date','dc','channel'), all.x = T)

dataNew$TY_actual = dataNew$volume
dataNew$volume = NULL

# This bit will allow for someone to manually adjust the forecast
# It reads in an xlsx file that acts as the interface for manual adjustments
adjustments = openxlsx::read.xlsx(xlsxFile = '//cants802/share2/SHARE/Logistics/Operations Planning Forecasts/Forecast_Manual_Adjustments.xlsx', sheet = 1, detectDates = T)
adjustments = adjustments[!is.na(adjustments$from_date),]
adjustments$to_date[is.na(adjustments$to_date)] = as.Date('2020/1/1')
names(adjustments)[names(adjustments) == 'X..or..'] = 'type'

adjustments = subset(adjustments, subset = from_date > as.Date(base::date(),'') | to_date > as.Date(base::date(), ''))
if(nrow(adjustments) != 0){
  for(i in 1:nrow(adjustments)){
    dateRange = seq.Date(from = max(adjustments[i,'from_date'], as.Date(base::date(),'')), to = adjustments[i,'to_date'], by = 1)
    if(adjustments$type[i] == "+"){
      dataNew$TY_forecast[dataNew$TY_date %in% dateRange & dataNew$dc == adjustments$dc[i] & dataNew$channel == adjustments$channel[i]] =
        dataNew$TY_forecast[dataNew$TY_date %in% dateRange & dataNew$dc == adjustments$dc[i] & dataNew$channel == adjustments$channel[i]] + adjustments$change[i]
    }
    else if(adjustments$type[i] == "*"){
      dataNew$TY_forecast[dataNew$TY_date %in% dateRange & dataNew$dc == adjustments$dc[i] & dataNew$channel == adjustments$channel[i]] =
        dataNew$TY_forecast[dataNew$TY_date %in% dateRange & dataNew$dc == adjustments$dc[i] & dataNew$channel == adjustments$channel[i]] * adjustments$change[i]
    }else if(adjustments$type[i] == "="){
      dataNew$TY_forecast[dataNew$TY_date %in% dateRange & dataNew$dc == adjustments$dc[i] & dataNew$channel == adjustments$channel[i]] = adjustments$change[i]
    }
  }}

# Compute variance column
dataNew$variance = (dataNew$TY_forecast - dataNew$TY_actual)/dataNew$TY_actual

# Make sure the ordering of the columns is the same as the original xlsx file
# and write to file for Tableau to pick up
dataNew = dataNew[,names(dataOld)]
dataNew = dataNew[order(dataNew$dc, dataNew$channel, dataNew$TY_date),]
data7072=dataNew[dataNew$dc=='7072',]
data7072$TY_forecast=ifelse(data7072$TY_date >= as.Date(base::date(),''),
                            data7072$TY_forecast*1.05, data7072$TY_forecast)
dataNew=dataNew[dataNew$dc!='7072',]
dataNew =merge(dataNew, data7072,all.x=T,all.y=T)
dataNew$TY_forecast=round(dataNew$TY_forecast)
##View(dataNew[dataNew$dc=='7072',])
### WRITE OUT ###
gc()
xlsx::write.xlsx2(dataNew, file = paste0('//cants802/share2/SHARE/Logistics Analytics/Forecast/Output/exportToTableau',as.Date(base::date(),''),'.xlsx'), sheetName = 'Data', row.names = F)
xlsx::write.xlsx2(dataNew, file = paste('//cants802/share2/SHARE/Logistics/Operations Planning Forecasts/Forecast.xlsx', sep = ''), sheetName = 'Data', row.names = F)
#################
### Additional Write Outs for CA Logistics ###

# This will split the final output into one data frame per DC and save
# to a shared folder so that CA Logistics can email to the 3PL DCs
# (i.e. 7072, 6601, and 6064P). These sites have no access to Tableau
set.seed(123)

data3PL =split(dataNew, f = dataNew$dc)
for(i in seq_along(data3PL)){
  print(names(data3PL[i]))
  gc()
  xlsx::write.xlsx(x = data3PL[[i]], 
                       file = paste0("//cadfs/logistics/Logistics/PDC Forecast/Forecasting/Tomorrows Forecasts/DC_", names(data3PL)[i],'.xlsx'))
  if(names(data3PL[i])=='6601'){
    attachmentPath = paste0("//cadfs/logistics/Logistics/PDC Forecast/Forecasting/Tomorrows Forecasts/DC_", names(data3PL)[i],'.xlsx')
    attachmentName = paste0(names(data3PL)[i],'.xlsx')
    attachmentObject <- mime_part(x=attachmentPath,name=attachmentName)
    from <- "<samuel.tisi@walmart.com>"
    to <- c('<Zhe.Li@walmart.com>','<Simon.Hamel@Relais-Logistique.com>', '<Constantin.Cristea@Relais-Logistique.com>', '<maya.kumarapeli@relais-logistique.com>', '<drew.robertson@walmartlogistics.ca>', '<Troy.Mascarenhas@walmartlogistics.ca>', '<Rhys.Fernandez@walmart.com>', '<priya.sharma@walmartlogistics.ca>', '<CALogistics.Operations@wal-mart.com>', '<faltoma@wal-mart.com>',' <Julie.Engdahl@Relais-Logistique.com>')

    subject <- paste0("Forecast Result ", as.Date(base::date(),''))
    body<-("Please find attached the updated forecast for today.")
    bodyWithAttachment <- list(body,attachmentObject)
    mailControl=list(smtpServer="smtp-gw1.wal-mart.com")
    for (j in seq_along(to)){
      sendmail(from=from,to=to[j],subject=subject,msg=bodyWithAttachment,control=mailControl)
    }
  }
  if(names(data3PL[i])=='6064'){
    attachmentPath = paste0("//cadfs/logistics/Logistics/PDC Forecast/Forecasting/Tomorrows Forecasts/DC_", names(data3PL)[i],'.xlsx')
    attachmentName = paste0(names(data3PL)[i],'.xlsx')
    attachmentObject <- mime_part(x=attachmentPath,name=attachmentName)
    from <- "<samuel.tisi@walmart.com>"
    to <- c('<Zhe.Li@walmart.com>','<andrew@cdsltd.ca>', '<collin@cdsltd.ca>', '<dc6064richmond@cdsltd.ca>', '<jamie.Baker@walmartlogistics.ca>', '<derek_lachaine@transx.com>', '<rusty_myskiw@transx.com>', '<Michael.Buna@walmart.com>', '<Troy.Mascarenhas@walmartlogistics.ca>', '<CALogistics.Operations@wal-mart.com>',' <faltoma@wal-mart.com>',' <andres@cdsltd.ca>')
    subject <- paste0("Forecast Result ", as.Date(base::date(),''))
    body<-("Please find attached the updated forecast for today.")
    bodyWithAttachment <- list(body,attachmentObject)
    mailControl=list(smtpServer="smtp-gw1.wal-mart.com")
    for (j in seq_along(to)){
      sendmail(from=from,to=to[j],subject=subject,msg=bodyWithAttachment,control=mailControl)
    }
  }
  if(names(data3PL[i])=='7072'){
    print("something is defitiong hasdhdf")
    attachmentPath = paste0("//cadfs/logistics/Logistics/PDC Forecast/Forecasting/Tomorrows Forecasts/DC_", names(data3PL)[i],'.xlsx')
    attachmentName = paste0(names(data3PL)[i],'.xlsx')
    attachmentObject <- mime_part(x=attachmentPath,name=attachmentName)
    from <- "<samuel.tisi@walmart.com>"
    to <- c('<Zhe.Li@walmart.com>','<tonys@trencold.com>', '<johnb@trencold.com>', '<dc7072trenton@trencold.com>', '<Lorn.Tootle@walmartlogistics.ca>', '<billw@trencold.com>', '<davej@trencold.com>', '<kylea@trencold.com>', '<tcagsupervisors@trencold.com>', '<drew.robertson@walmartlogistics.ca>',' <derek_lachaine@transx.com>',' <mikepax@trencold.com>',' <jgarracha@transx.com>',' <Mike_Przygocki@transx.com>',' <rusty_myskiw@transx.com>',' <ktarka@eassons.com>',' <kjacobs@eassons.com>',' <Peter@eassons.com>',' <CLaking@erbgroup.com>',' <JVerkuyl@erbgroup.com>',' <Michael.Buna@walmart.com>',' <Troy.Mascarenhas@walmartlogistics.ca>',' <CALogistics.Operations@wal-mart.com>',' <faltoma@wal-mart.com>',' <derek_lachaine@transx.com>')
    subject <- paste0("Forecast Result ", as.Date(base::date(),''))
    body<-("Please find attached the updated forecast for today.")
    bodyWithAttachment <- list(body,attachmentObject)
    mailControl=list(smtpServer="smtp-gw1.wal-mart.com")
    for (j in seq_along(to)){
      sendmail(from=from,to=to[j],subject=subject,msg=bodyWithAttachment,control=mailControl)
    }

  }

  }

# This will get tomorrows forecasts for CA Logistics
dataTomorrow = subset(dataNew, subset = TY_date == as.Date(date(),'')+1,
                      select = c(dc,channel,TY_forecast))
dataTomorrow = dataTomorrow[c(8:11,2:3,5,7,1,4,6),]

gc()
xlsx::write.xlsx(x = dataTomorrow, 
                 file = paste0("//cadfs/logistics/Logistics/PDC Forecast/Forecasting/Tomorrows Forecasts/Forecasts_for_", as.Date(date(),'')+1,'.xlsx'),
                 row.names = F)
              

print("Program complete.")
